{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大作业要求"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.大作业以2-3人为一组完成，提交材料包括PPT（最后一次课将会展示课程成果）+ 最终的大作业报告（需组内各成员单独提交，内容为本人在课程大作业中的贡献以及对大作业问题的思考) + 提交包含分工情况及组内各成员工作量占比的表格。分工表格需组内所有成员签字确认；\n",
    "\n",
    "2.禁止抄袭，发现雷同，所有雷同提交分数除以2；\n",
    "\n",
    "3.写清楚大作业中的贡献和创新点，若使用开源代码和论文中的方法，在报告中必须注明（不可作为本人创新点），发现不标注引用，分数除以2。\n",
    "\n",
    "最后一次课展示说明：\n",
    "1.样例\n",
    "PPT例子：https://www.sohu.com/a/166633625_642762\n",
    "2.展示时间限制：\n",
    "展示时间为最后一节课，展示时间为6分钟讲+2分钟同学助教老师自由提问\n",
    "\n",
    "大作业报告：强调个人对问题的理解，以及贡献，建议增加在提问反馈之后的改进结果。\n",
    "\n",
    "最终评分为:30%展示评分+70%大作业报告"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题描述(Out-of-Distribution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度神经网络通常采用独立同分布(Independent-Identically)的假设进行训练，即假设测试数据分布与训练数据分布相似。然而，当用于实际任务时，这一假设并不成立，导致其性能显著下降。虽然这种性能下降对于产品推荐等宽容性大的应用是可以接受的，但在医学等宽容性小的领域使用此类系统是危险的，因为它们可能导致严重事故。理想的人工智能系统应尽可能在分布外（Out-of-Distribution）的情况下有较强的分部外泛化能力。而提高分布外泛化的关键点，就是如何让模型学习到数据中的causal feature。  \n",
    "一个简单的例子：以猫狗二分类为例，如果训练集中所有狗都在草地上，所有的猫都在沙发上，而测试集中所有的狗在沙发上，所有的猫在草地上，那么模型在没有测试集信息的情况下，很有可能根据训练集的信息把草地和狗联系在了一起，沙发和猫联系在了一起，当模型在测试集上测试时将会把在沙发上的狗误认为是猫。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集(Colored MNIST)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colored MNIST是一个分布外泛化领域中常用的数据集，在该数据集中，训练集和测试集之间存在Out-of-Distribution情况，color feature和数字产生了spurious correlation，即虚假的因果关系。从直观上来说，数字的形状为causal feature，数字的颜色为non-causal feature。该次大作业旨在探索如何让模型学习到causal feature来提高泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.datasets.utils as dataset_utils\n",
    "from torch.utils.data import DataLoader\n",
    "from Lenet import LeNet\n",
    "from Lenet import LeNet_NoBN\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_grayscale_arr(arr, red=True):\n",
    "    \"\"\"Converts grayscale image to either red or green\"\"\"\n",
    "    assert arr.ndim == 2\n",
    "    dtype = arr.dtype\n",
    "    h, w = arr.shape\n",
    "    arr = np.reshape(arr, [h, w, 1])\n",
    "    if red:\n",
    "        arr = np.concatenate([arr,\n",
    "                              np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
    "    else:\n",
    "        arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
    "                              arr,\n",
    "                              np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class ColoredMNIST(datasets.VisionDataset):\n",
    "    \"\"\"\n",
    "  Colored MNIST dataset for testing IRM. Prepared using procedure from https://arxiv.org/pdf/1907.02893.pdf\n",
    "\n",
    "  Args:\n",
    "    root (string): Root directory of dataset where ``ColoredMNIST/*.pt`` will exist.\n",
    "    env (string): Which environment to load. Must be 1 of 'train1', 'train2', 'test', or 'all_train'.\n",
    "    transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "      and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "    target_transform (callable, optional): A function/transform that takes in the\n",
    "      target and transforms it.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, root='./data', env='train1', transform=None, target_transform=None):\n",
    "        super(ColoredMNIST, self).__init__(root, transform=transform,\n",
    "                                           target_transform=target_transform)\n",
    "\n",
    "        self.prepare_colored_mnist()\n",
    "        if env in ['train1', 'train2', 'test']:\n",
    "            self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt')\n",
    "        elif env == 'all_train':\n",
    "            self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', 'train1.pt')) + \\\n",
    "                                     torch.load(os.path.join(self.root, 'ColoredMNIST', 'train2.pt'))\n",
    "        else:\n",
    "            raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, test, and all_train')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "    Args:\n",
    "        index (int): Index\n",
    "\n",
    "    Returns:\n",
    "        tuple: (image, target) where target is index of the target class.\n",
    "    \"\"\"\n",
    "        img, target = self.data_label_tuples[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_label_tuples)\n",
    "\n",
    "    def get_label(self):\n",
    "        return np.array(self.data_label_tuples)[:, 1]\n",
    "\n",
    "    def prepare_colored_mnist(self):\n",
    "        colored_mnist_dir = os.path.join(self.root, 'ColoredMNIST')\n",
    "        if os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'test.pt')):\n",
    "            print('Colored MNIST dataset already exists')\n",
    "            return\n",
    "\n",
    "        print('Preparing Colored MNIST')\n",
    "        train_mnist = datasets.mnist.MNIST(self.root, train=True, download=True)\n",
    "\n",
    "        train1_set = []\n",
    "        train2_set = []\n",
    "        test_set = []\n",
    "        for idx, (im, label) in enumerate(train_mnist):\n",
    "            if idx % 10000 == 0:\n",
    "                print(f'Converting image {idx}/{len(train_mnist)}')\n",
    "            im_array = np.array(im)\n",
    "\n",
    "            # Assign a binary label y to the image based on the digit\n",
    "            binary_label = 0 if label < 5 else 1\n",
    "\n",
    "            # Flip label with 25% probability\n",
    "            if np.random.uniform() < 0.25:\n",
    "                binary_label = binary_label ^ 1\n",
    "\n",
    "            # Color the image either red or green according to its possibly flipped label\n",
    "            color_red = binary_label == 0\n",
    "\n",
    "            # Flip the color with a probability e that depends on the environment\n",
    "            if idx < 20000:\n",
    "                # 20% in the first training environment\n",
    "                if np.random.uniform() < 0.2:\n",
    "                    color_red = not color_red\n",
    "            elif idx < 40000:\n",
    "                # 10% in the first training environment\n",
    "                if np.random.uniform() < 0.1:\n",
    "                    color_red = not color_red\n",
    "            else:\n",
    "                # 90% in the test environment\n",
    "                if np.random.uniform() < 0.9:\n",
    "                    color_red = not color_red\n",
    "\n",
    "            colored_arr = color_grayscale_arr(im_array, red=color_red)\n",
    "\n",
    "            if idx < 20000:\n",
    "                train1_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 40000:\n",
    "                train2_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            else:\n",
    "                test_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "\n",
    "        if not os.path.exists(colored_mnist_dir):\n",
    "            os.makedirs(colored_mnist_dir)\n",
    "        torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
    "        torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
    "        torch.save(test_set, os.path.join(colored_mnist_dir, 'test.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初级部分：数据预处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.在Colored MNIST上训练和测试LeNet。  \n",
    "2.在数据读取过程中增加数据预处理的方式（数据增广等），提高OOD泛化能力 【若选做高级部分可以跳过】\n",
    "\n",
    "【OOD算法性能评价准则：在训练过程中只能接触训练集，不能在测试集上进行调参或者模型选择】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.manual_seed(1212)\n",
    "\n",
    "def get_gaussian_noise(img):\n",
    "    mean = 0\n",
    "    std = 2\n",
    "    noise = np.random.normal(mean, std, img.shape)\n",
    "    noisy_img = np.clip(img+noise,0,255)\n",
    "    return noisy_img\n",
    "\n",
    "def get_colored_pixel(img):\n",
    "    tmp = np.nonzero(img)\n",
    "    x = img[:, tmp[0][1], tmp[0][2]].reshape(3, 1, 1)\n",
    "    x = np.broadcast_to(x, (3, 28, 28))\n",
    "    return torch.tensor(x)\n",
    "\n",
    "def get_mean(img):\n",
    "    x = torch.mean(img, axis=0)\n",
    "    x = torch.tensor(np.broadcast_to(x, (3,28,28)))\n",
    "    return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.CenterCrop(7), \n",
    "    # transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.ToTensor(), \n",
    "    # transforms.Lambda(lambda img: get_mean(img)), \n",
    "    # transforms.Lambda(lambda img: get_gaussian_noise(img)), \n",
    "    transforms.Normalize((0.5, ), (0.5, )), \n",
    "    transforms.Resize([28, 28])\n",
    "])\n",
    "train_data = ColoredMNIST(env='all_train', transform=transform)\n",
    "test_data = ColoredMNIST(env='test', transform=transforms.ToTensor())\n",
    "num_train = 35000\n",
    "train_loader = DataLoader(train_data, batch_size=128, \n",
    "                          sampler=torch.utils.data.SubsetRandomSampler(range(num_train)))\n",
    "val_loader = DataLoader(train_data, batch_size=128, \n",
    "                        sampler=torch.utils.data.SubsetRandomSampler(range(num_train, 40000)))\n",
    "test_loader = DataLoader(test_data, batch_size=2000, shuffle=True)\n",
    "# print(train_data[0][0].shape)\n",
    "# input shape is 3*28*28\n",
    "train1 = ColoredMNIST(env='train1', transform=transform)\n",
    "train2 = ColoredMNIST(env='train2', transform=transform)\n",
    "dtype = train1[0][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of part of the dataset\n",
    "plt.figure(figsize=(5,16))\n",
    "for i, ds in enumerate(['train1', 'train2', 'test_data']):\n",
    "    for j in range(2):\n",
    "        label = eval(ds).get_label()\n",
    "        index = np.flatnonzero(label == j)\n",
    "        index = np.random.choice(index, 20, replace=False)\n",
    "        for k in range(5):\n",
    "            if k==1:\n",
    "                if j==0:\n",
    "                    plt.title('red')\n",
    "                if j==1:\n",
    "                    plt.title('green')\n",
    "            plt.subplot(20, 8, k*8 + j + 3*i + 1)\n",
    "            plt.imshow(eval(ds)[index[k]][0].permute(1, 2, 0))\n",
    "            plt.axis('off')\n",
    "    \n",
    "    for p in range(20):\n",
    "        plt.subplot(20, 8, 8*k + 3*i)\n",
    "        plt.axis('off')\n",
    "    plt.figtext(0.22 + i * 0.3, 0.91, s=ds, ha='center', fontsize=12, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('std5.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for train1 and train2\n",
    "num_train12 = 20000\n",
    "train1_loader = DataLoader(train1, batch_size=20000, shuffle=False)\n",
    "train2_loader = DataLoader(train2, batch_size=20000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(scores, y):\n",
    "    y_pred = scores > 0\n",
    "    return (y_pred == y).float().mean()\n",
    "\n",
    "def data_split(data, device, dtype=torch.float32):\n",
    "    x1, y1 = data[0]\n",
    "    x2, y2 = data[1]\n",
    "    x3, y3 = data[2]\n",
    "    x1 = x1.to(device,dtype)\n",
    "    x2 = x2.to(device,dtype)\n",
    "    x3 = x3.to(device, dtype)\n",
    "    y1 = y1.to(device,dtype).unsqueeze(1)\n",
    "    y2 = y2.to(device,dtype).unsqueeze(1)\n",
    "    y3 = y3.to(device,dtype).unsqueeze(1)\n",
    "    return x1, y1, x2, y2, x3, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to GPU\n",
    "data = list(zip(train1_loader, train2_loader, test_loader))[0]\n",
    "x1, y1, x2, y2, x3, y3 = data_split(data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_history_erm = {}\n",
    "test_acc_history_erm = {}\n",
    "dummy = torch.nn.Parameter(torch.tensor(1.0), requires_grad=True).to(device)\n",
    "environments=['train1', 'train2']\n",
    "def train_IRM(model, optimizer, epoch, dummy, \n",
    "              penalty_anneal_iters=20000, lamb=1e4):\n",
    "    model.train()\n",
    "    x = torch.cat((x1, x2), dim=0)\n",
    "    y = torch.cat((y1, y2), dim=0)\n",
    "    for e in range(epoch):\n",
    "        scores = model(x)\n",
    "        scores3 = model(x3)\n",
    "        train_acc = get_acc(scores, y)\n",
    "        test_acc = get_acc(scores3, y3)\n",
    "        loss = F.binary_cross_entropy_with_logits(scores, y)\n",
    "        total_loss = loss \n",
    "        train_acc_history_erm[e] = train_acc.detach().cpu()\n",
    "        test_acc_history_erm[e] = test_acc.detach().cpu()\n",
    "        if e == penalty_anneal_iters:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        print(\"Epoch {},total_loss: {:3f}, tran_acc: {:3f}, test_acc: {:3f}\"\n",
    "            .format(e, loss, train_acc, test_acc))\n",
    "        if e%10 == 0:\n",
    "            print('------------------------')\n",
    "    \n",
    "\n",
    "# trianing time!\n",
    "learning_rate = 4e-3\n",
    "model11 = LeNet(out_channels=1).to(device=device)\n",
    "optimizer = optim.Adam(model11.parameters(), lr=learning_rate)\n",
    "train_IRM(model11, optimizer, epoch=300, dummy=dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "train_curve, = plt.plot(list(train_acc_history_erm.values()))\n",
    "test_curve, = plt.plot(list(test_acc_history_erm.values()))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(handles=[train_curve, test_curve], \n",
    "           labels=['train_acc', 'test_acc'], loc='best')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中级部分：算法复现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/DomainBed  \n",
    "1.复现Invariant Risk Minimization (IRM)算法。  \n",
    "2.从以下论文中选择一个OOD算法复现，思考什么样的算法可以在此数据集上取得较好效果。  \n",
    "    - Domain-Adversarial Training of Neural Networks (DANN)    \n",
    "    - Out-of-Distribution Generalization via Risk Extrapolation (VREx)  \n",
    "    - Learning Explanations that are Hard to Vary (AndMask)  \n",
    "    - Self-Challenging Improves Cross-Domain Generalization (RSC)   \n",
    "3.IRM算法对penalty weight参数较为敏感，如何改进，提高IRM算法稳定性。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRM复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_penalty(scores, y, dummy):\n",
    "    loss = F.binary_cross_entropy_with_logits(scores * dummy, y)\n",
    "    g = grad(loss, dummy, create_graph=True)[0]\n",
    "    return (g**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_IRM(model, optimizer, epoch, dummy, \n",
    "              penalty_anneal_iters=50, lamb=1e3, alpha=0.9):\n",
    "    model.train()\n",
    "    last_trian_acc = 0.5\n",
    "    penalty_weight = lamb\n",
    "    current_weight = lamb\n",
    "    beta = 1\n",
    "    gamma = 1\n",
    "    train_acc_history_irm = {}\n",
    "    test_acc_history_irm = {}\n",
    "    for e in range(epoch):\n",
    "        scores1 = model(x1)\n",
    "        scores2 = model(x2)\n",
    "        scores3 = model(x3)\n",
    "        train_acc = (get_acc(scores1, y1) + get_acc(scores2, y2)) / 2\n",
    "        test_acc = get_acc(scores3, y3)\n",
    "        penalty = (compute_penalty(scores1, y1, dummy) \\\n",
    "                + compute_penalty(scores2, y2, dummy)) / 2\n",
    "        loss = (F.binary_cross_entropy_with_logits(scores1, y1)\\\n",
    "                + F.binary_cross_entropy_with_logits(scores2, y2)) / 2\n",
    "        \n",
    "        penalty_weight = lamb if e>penalty_anneal_iters else 1.0\n",
    "        total_loss = loss + penalty_weight * penalty\n",
    "        train_acc_history_irm[e] = train_acc.detach().cpu()\n",
    "        test_acc_history_irm[e] = test_acc.detach().cpu()\n",
    "        if e == penalty_anneal_iters:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=7e-4)\n",
    "        if penalty_weight > 1:\n",
    "            total_loss /= penalty_weight\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(\"Epoch {},total_loss: {:3f}, tran_acc: {:3f}, test_acc: {:3f}, \"\n",
    "              \"penalty_weight:{:3f}, current_weight:{:3f}, beta:{:3f}\"\n",
    "               .format(e, penalty_weight * penalty, train_acc, test_acc, penalty_weight,current_weight,beta))\n",
    "        \n",
    "\n",
    "    return train_acc_history_irm, test_acc_history_irm\n",
    "\n",
    "# trianing time!\n",
    "learning_rate = 3e-3\n",
    "model = LeNet(out_channels=1).to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=7e-4)\n",
    "train_acc_history_irm, test_acc_history_irm = train_IRM(model, optimizer, epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.subplot(2,1,1)\n",
    "train_curve, = plt.plot(list(train_acc_history_irm.values()))\n",
    "test_curve, = plt.plot(list(test_acc_history_irm.values()))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(handles=[train_curve, test_curve], \n",
    "           labels=['train_acc', 'test_acc'], loc='best')\n",
    "plt.subplot(2,1,2)\n",
    "train_curve, = plt.plot(list(train_acc_history_irm.values())[100:])\n",
    "test_curve, = plt.plot(list(test_acc_history_irm.values())[100:])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(handles=[train_curve, test_curve], \n",
    "           labels=['train_acc', 'test_acc'], loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VREx复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_history_VREx = {}\n",
    "test_acc_history_VREx = {}\n",
    "def train_VERx(model, optimizer, epoch, \n",
    "              penalty_anneal_iters=100, lamb=1e4):\n",
    "    model.train()\n",
    "    for e in range(epoch + 1):\n",
    "        scores1 = model(x1)\n",
    "        scores2 = model(x2)\n",
    "        scores3 = model(x3)\n",
    "        train_acc = (get_acc(scores1, y1) + get_acc(scores2, y2)) / 2\n",
    "        test_acc = (get_acc(scores3, y3))\n",
    "        loss1 = F.binary_cross_entropy_with_logits(scores1, y1)\n",
    "        loss2 = F.binary_cross_entropy_with_logits(scores2, y2)\n",
    "        penalty = (loss1 - loss2) ** 2\n",
    "        loss = (loss1 + loss2)\n",
    "        \n",
    "        penalty_weight = lamb if e>penalty_anneal_iters else 1.0\n",
    "        total_loss = loss + penalty_weight * penalty\n",
    "        if penalty_weight > 1:\n",
    "            total_loss /= penalty_weight\n",
    "\n",
    "        if e == penalty_anneal_iters:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=7e-4)\n",
    "        train_acc_history_VREx[e] = train_acc.detach().cpu()\n",
    "        test_acc_history_VREx[e] = test_acc.detach().cpu()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        print(\"Epoch {},total_loss: {:3f}, tran_acc: {:3f}, test_acc: {:3f}\"\n",
    "            .format(e, penalty_weight * penalty, train_acc, test_acc))\n",
    "\n",
    "# trianing time!\n",
    "learning_rate = 3e-3\n",
    "model = LeNet(out_channels=1).to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=7e-4)\n",
    "train_VERx(model, optimizer, epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(2,1,1)\n",
    "train_curve, = plt.plot(list(train_acc_history_VREx.values()))\n",
    "test_curve, = plt.plot(list(test_acc_history_VREx.values()))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(handles=[train_curve, test_curve], \n",
    "           labels=['train_acc', 'test_acc'], loc='best')\n",
    "plt.subplot(2,1,2)\n",
    "train_curve, = plt.plot(list(train_acc_history_VREx.values())[100:])\n",
    "test_curve, = plt.plot(list(test_acc_history_VREx.values())[100:])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend(handles=[train_curve, test_curve], \n",
    "           labels=['train_acc', 'test_acc'], loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalty Fluctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.nn.Parameter(torch.tensor(1.0), requires_grad=True).to(device)\n",
    "def train_IRM_fluc(model, optimizer, epoch, dummy, \n",
    "              penalty_anneal_iters=50, lamb=1e3, alpha=0.9):\n",
    "    train_acc_history_irm = {}\n",
    "    test_acc_history_irm = {}\n",
    "    model.train()\n",
    "    last_trian_acc = 0.5\n",
    "    penalty_weight = lamb\n",
    "    current_weight = lamb\n",
    "    beta = 1\n",
    "    gamma = 1\n",
    "    for e in range(epoch):\n",
    "        scores1 = model(x1)\n",
    "        scores2 = model(x2)\n",
    "        scores3 = model(x3)\n",
    "        train_acc = (get_acc(scores1, y1) + get_acc(scores2, y2)) / 2\n",
    "        test_acc = get_acc(scores3, y3)\n",
    "        penalty = (compute_penalty(scores1, y1, dummy) \\\n",
    "                + compute_penalty(scores2, y2, dummy)) / 2\n",
    "        loss = (F.binary_cross_entropy_with_logits(scores1, y1)\\\n",
    "                + F.binary_cross_entropy_with_logits(scores2, y2)) / 2\n",
    "        # Penalty Fluctuation\n",
    "        if e<penalty_anneal_iters:\n",
    "            penalty_weight = 1\n",
    "        elif e==penalty_anneal_iters:\n",
    "            last_trian_acc = train_acc\n",
    "            penalty_weight = lamb\n",
    "        else:\n",
    "            beta = last_trian_acc / train_acc\n",
    "            current_weight = lamb * 1e4**(-torch.tanh(beta**2 - 1))\n",
    "            penalty_weight = alpha * penalty_weight + (1-alpha) * current_weight\n",
    "            last_trian_acc = train_acc\n",
    "        \n",
    "        total_loss = loss + penalty_weight * penalty\n",
    "        train_acc_history_irm[e] = train_acc.detach().cpu()\n",
    "        test_acc_history_irm[e] = test_acc.detach().cpu()\n",
    "        if e == penalty_anneal_iters:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=7e-4)\n",
    "        if penalty_weight > 1:\n",
    "            total_loss /= penalty_weight\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(\"Epoch {},total_loss: {:3f}, tran_acc: {:3f}, test_acc: {:3f},\"\n",
    "              \"penalty_weight:{:3f}, current_weight:{:3f}, beta:{:3f}\"\n",
    "          .format(e, penalty_weight * penalty, train_acc, test_acc, penalty_weight,current_weight,beta))\n",
    "\n",
    "    return train_acc_history_erm, test_acc_history_irm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambs = np.arange(1e5, 1e6 + 1, 1e5)\n",
    "converged_acc_nofluc = []\n",
    "converged_acc_fluc = []\n",
    "for lamb in lambs:\n",
    "    model1 = LeNet_NoBN(out_channels=1).to(device=device)\n",
    "    model2 = LeNet_NoBN(out_channels=1).to(device=device)\n",
    "    learning_rate = 4e-3\n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate, weight_decay=7e-4)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate, weight_decay=7e-4)\n",
    "    _, test1 = train_IRM(model1, optimizer1, epoch=100, dummy=dummy, lamb=lamb)\n",
    "    acc = list(test1.values())[97]\n",
    "    converged_acc_nofluc.append(acc)\n",
    "    print(\"lamb:{:e}, finish training model1:{:3f}\".format(lamb, acc))\n",
    "    _, test2 = train_IRM_fluc(model2, optimizer2, epoch=100, dummy=dummy, lamb=lamb)\n",
    "    acc = list(test2.values())[98]\n",
    "    converged_acc_fluc.append(acc)\n",
    "    print(\"lamb:{:e}, finish training model2: {}\".format(lamb, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
